---
layout: post
author: Lionel Pigou
title: "Encoder-Decoder Language Model for Sign Language Recognition (SLR)"
tags: [machine learning, sign language recognition]
date: 2018-04-01
---


Disclaimer: This post is summarized by AI based on the chapter in my Ph.D dissertation on [page 155](/assets/phd-lionelpigou.pdf#page=155). 

<!-- ![theme logo](/assets/images/projects/vrt2.gif) -->
<!-- <img src="/assets/images/projects/vrt2.gif" alt="img" />
*image caption* -->
<!-- <center>Centered text</center> -->
<!-- <div class="figure"> -->

| ![space-1.jpg](/assets/images/projects/vrt2.gif) | 
|:--:| 
| *image caption* |


TV broadcasting organizations like the BBC (British Broadcasting Corporation) or the VRT (Flemish Radio and Television Broadcasting Organization) are making news broadcasts accessible to deaf people by overlaying a sign language interpreter on the screen. This creates a large amount of data where spoken language is interpreted into sign language. However, translating sign language to written Dutch is challenging due to the lack of examples and the lack of a one-to-one mapping between sign language and written language. In this chapter, we explore the problem of sign language recognition in TV news broadcasts and propose a model that aligns subtitles with sign language video fragments.


## VRT news dataset
We collected a dataset of news videos with Flemish Sign Language interpreter overlays in collaboration with VRT. The dataset contains daily news broadcasts from September 2012 to July 2015, totaling 575 hours of usable and subtitled footage. The interpreter overlay is mixed with the background news broadcast, and there are four different interpreters and two different static backgrounds in the dataset.

## Methodology
Our approach involves embedding the source sign language video fragments into a shared vector space using a convolutional neural network (CNN). We leverage an existing Word2Vec model trained on Dutch words to encode the target subtitles into the same vector space. We then calculate the matching scores between the source video fragments and the target words using dot products in the embedding vector space. To train the model, we employ a ranking-based objective that aims to maximize the similarity between the video representation and the target subtitles while minimizing the similarity with random subtitles.

## Experiments
We preprocess the data by cropping the video frames, converting them to grayscale, and subtracting the previous frame to remove static information. We also filter and clean the subtitles, using a pretrained Word2Vec model for word embeddings. The dataset is split into training, validation, and test sets. We train the model using the Adam update rule and orthogonal weight initialization. The margin loss converges to around 0.9, and the accuracy of matching positive and negative targets reaches approximately 74%.

## Results
The model shows promising results in predicting the general theme of the video fragments in some cases, with an estimated 25% rate of sensible results in terms of topic. The model also learns certain key points in the vector space, with specific neighboring words consistently emerging for different topics. However, the model's performance is still far from achieving accurate translation between sign language and written Dutch.

## Conclusion
We have developed a model that embeds sign language video fragments into a shared vector space with Dutch subtitles. While the model shows some ability to predict the topic of video fragments and learns specific patterns, the overall results are not yet at the level of accurate translation. Future improvements could involve hand tracking, using pretrained language models for encoding groups of words or sentences, incorporating sign language corpora in training, and exploring larger models.

<!-- Many TV broadcasting organizations like the BBC (British Broadcasting Corporation) or the VRT (Flemish Radio and Television Broadcasting Organization) are making their news broadcasts accessible to deaf people by overlaying an interpreter on the screen. This provides a significant amount of data where spoken language is interpreted into sign language. This data presents a unique and challenging machine translation or video captioning problem, where the video stream serves as the source and the subtitles as the targets.

Traditionally, sign language recognition has focused on transcribing individual signs separately into glosses. However, translating these glosses into written Dutch is not a straightforward task. There are few examples available where glosses are converted into Dutch sentences, and sign language and written language do not have a one-to-one mapping on the word level. Although there is no direct mapping between sign language and written Dutch on a word level, there is a mapping of meaning. A short sign language sequence can be mapped to the meaning of a word, a group of Dutch words, or even a Dutch sentence. This understanding of the problem allows us to develop models that bridge the gap between sign language and written language.

# Challenges: Alignment and Translation

There are two primary challenges in this task. The first challenge is aligning the subtitles to the sign language interpreter, as the interpreter's timing is not perfectly synchronized with the subtitles. The second challenge is translating Flemish Sign Language into written Dutch. To align the subtitles, one needs to understand Flemish Sign Language, and to translate the videos, one needs to align the subtitles. This presents a seemingly impossible task at first glance. However, similar problems in other fields have been successfully solved using joint detection and classification models, such as object detection and classification in images or aligning and translating in neural machine translation.

# VRT News Dataset

To address these challenges, we collected a large dataset of news videos with Flemish Sign Language interpreter overlays in collaboration with VRT. The dataset spans from September 2012 to July 2015 and includes the daily news broadcasts at 7 PM. The data collection process involved building web crawlers to acquire video footage, subtitles, and metadata linking them together. After filtering and cleaning the data, we ended up with 891 usable broadcast-subtitles matches out of 947 video files, totaling 575 hours of subtitled footage.

# Methodology

Our approach involves embedding the source video fragments and target words into a shared vector space. We use a convolutional neural network (CNN) to encode the video fragments into a vector representation, and a pretrained Word2Vec model to represent the target words in the vector space. By calculating the matching scores between the source and target words in this shared vector space, we can identify the words that match the sign language in the video.

To train the model, we employ a ranking-based objective using a margin loss. This objective aims to maximize the similarity between the video representation and the positive target (matching words in subtitles) while minimizing the similarity with a negative target (random subtitle sequence). Since we don't have exact alignments, this unsupervised objective allows the model to learn the alignment based on the ranking of matching scores.

# Experimental Results

We trained the model using the VRT news dataset and evaluated its performance. The margin loss during the training phase converged to around 0.9, while the accuracy of $s_{\text{aggr}}^+ > s_{\text{aggr}}^-$ (positive target has greater similarity than negative target) reached a rate of approximately 0.74. Although the loss and accuracy values are not ideal, they demonstrate that the model is learning to rank the matching words and distinguish them from random subtitles.

To gain more insight into the model's performance, we conducted a qualitative analysis. We evaluated the video fragments along with their corresponding subtitles, the neighboring words in the embedding space, and the true Flemish Sign Language glosses. The results showed that the model can predict the general theme of the fragment in some cases, with an estimated 25% rate of fragments having sensible topic predictions. Additionally, the embedding space exhibited certain patterns, such as the emergence of common topic-related words.

# Conclusion and Future Work

In conclusion, we developed a model that aligns subtitles to Flemish Sign Language video fragments by embedding them into a shared vector space. Despite the significant challenges posed by the lack of aligned data, the model showed promising results in learning the alignment and predicting the topic of fragments. However, the translation quality is still far from practical application.

Future work could focus on improving the model's performance by incorporating hand tracking for better signal-to-noise ratio, using pretrained language models that encode groups of Dutch words or sentences, and leveraging sign language corpora in the training process. Additionally, exploring larger models with more available memory and suitable regularization techniques may yield further improvements. -->



